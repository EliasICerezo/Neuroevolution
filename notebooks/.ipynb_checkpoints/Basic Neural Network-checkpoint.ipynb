{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic Neural Network from scratch\n",
    "Nowadays, building a neural network in python is very easy. TensorFlow and Keras allow us to build in a few lines of code a net structure and using some of the included datasets we can train it without further complication.\n",
    "However, if we were to modify the behaviour of the neural network's training procedure, for example, optimizing the weights with a genetic algorithm, we can't use those tools since we are not able to modify the training sequence easily.  \n",
    "This notebook aims to explain how to build a neural network from scratch using backpropagation as weight optimization mechanism.  \n",
    "\n",
    "## Use case scenario\n",
    "A neural network can be represented from different points of view. In this notebook we want to represent it as a set of arrays that ensemble the weights and the biases of the network. Additionally we will add to this arrays some calculations necessary to run the backpropagation, but they don't scale over time. Depending on the complexity of the structure proposed, these variables will scale in number and size, so it must be taken in consideration prior to build a deep neural network.  \n",
    "The neural network that we are going to build in this notebook is a fully connected one, that means, that every neuron from one layer is connected to all the neurons of the following layer. \n",
    "The rest of the notebook contains, a specification of the neural network, explanation of the algorithm and a proof that the algorithm actually work as intended.\n",
    "\n",
    "## Basic neural network class\n",
    "In this section we will attend to the code that make the training with backpropagation possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configure Jupyter to use correctly the path related to the code\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from neuroevolution.activation_functions import sigmoid, sigmoid_der, relu, relu_der\n",
    "from neuroevolution.error_functions import MSE, crossentropy_loss\n",
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "class BasicNeuralNetwork:\n",
    "  \"\"\"Class that implements the basic behaviour of a neural network.\n",
    "  It represents the fully connected networks, so take into account that all the\n",
    "  links in between nodes will be found here.\n",
    "  It ensembles all the basics and it is generalized to match every architecure.\n",
    "  However it is important to know that since it will need to store derivatives\n",
    "  to calculate the backpropagation step, it may not be super eficient in memory\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, layers:list, num_of_classes:int, input_size:int, lr = 0.05,\n",
    "               activation_functs = None):\n",
    "    \"\"\"Contructor of the basic Neural Network Object\n",
    "\n",
    "    Keyword arguments:\n",
    "      layers -- List of numbers that ensemble the number of neurons od each layer\n",
    "      lr -- Stands for learning rate and it ts the size of the movement once you\n",
    "      start training it.\n",
    "    \"\"\" \n",
    "    self.layers = layers\n",
    "    self.learning_rate = lr\n",
    "    self.params = {}\n",
    "    self.loss = []\n",
    "    if layers[-1] != num_of_classes:\n",
    "      raise AttributeError(\"The number of classes should match the last layer\")\n",
    "    if layers[0] != input_size:\n",
    "      raise AttributeError(\"The input size should match the 1st layer\")\n",
    "    if len(self.layers) == 0:\n",
    "      raise AttributeError(\"Can't create a neural net without a single layer \")\n",
    "    self.activation_functs = activation_functs\n",
    "    if activation_functs is None:\n",
    "      self.activation_functs = [sigmoid for i in range(len(self.layers))]\n",
    "    self.initialize_weithts_and_biases()\n",
    "  \n",
    "  def initialize_weithts_and_biases(self):\n",
    "    \"\"\"Function that initialize weights and biases randomly. The random values\n",
    "    are not uniformly distributed which can cause a slower convergence.\n",
    "    \"\"\"\n",
    "    np.random.seed = 42\n",
    "    for i,e in enumerate(self.layers):\n",
    "      if i < len(self.layers)-1:\n",
    "        self.params['W{}'.format(i+1)] = np.random.randn(self.layers[i],\n",
    "                                                       self.layers[i+1])\n",
    "        self.params['b{}'.format(i+1)] = np.random.randn(self.layers[i+1])\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these lines what we have showed is how the structure is defined in terms of data structures. We can find layers, learning rate, params, activation functions and loss.\n",
    "\n",
    "* Layers: Layers is an integer list that contains the number of neurons in each layer.\n",
    "* Learning Rate: Learning rate os the size of the step in this configuration, it is the number that will tell how quickly the neural network stop searching through the solutions space\n",
    "* Params: A dictionary that stores the weights and biases vectors, that are numpy arrays, and the calculated variables for the backpropagation computing.\n",
    "* Loss: A list that stores every epoch loss. This vector will be super useful to see how the training process reduces the value of the objective function.\n",
    "* Activation Functions: A list containing the activation function associated to each layer.\n",
    "\n",
    "Additionally the code run some checks to be sure that the number of neurons in the network is correct.\n",
    "In addition, the weight initialization is random, and we use the traditional notation to label the weights and biases arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, inputs: np.ndarray, targets: np.ndarray, epochs: int):\n",
    "    \"\"\"Training method of the neural network.\n",
    "    It optimized the weights for a given input\n",
    "    Keyword Arguments:\n",
    "      inputs -- the features to be predicted by the neural network\n",
    "      targets -- the targets that match the features for this predictions.\n",
    "      epochs -- number of iterations of optimization that the neural network\n",
    "      will perform\n",
    "    \"\"\"  \n",
    "    for i in range(epochs):\n",
    "      y_hat = self.feed_forward(inputs)\n",
    "      loss = crossentropy_loss(targets,y_hat)\n",
    "      self.loss.append(loss)\n",
    "      self.backpropagation(inputs,y=targets,y_hat=y_hat)\n",
    "      self.__weight_updating()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we define the training loop. The iteration calculates the forward pass, calculates the loss, and backprogates the results.\n",
    "* In the forward pass we make the dot operation using the weights and the inputs, and using those results to calculate the following layer.\n",
    "* In the backpropagation calculation we use the loss value and learning rate to calculate the values used to update the weights.\n",
    "* Finally, we use those calculated values to update the weights.  \n",
    "\n",
    "In the following cells we are going to explain each of these methods more thoroughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(self, inputs):\n",
    "    \"\"\"Function that performs the forward pass through the neural network, it\n",
    "    computes the dot product between the features and the weights and then adds\n",
    "    the bias assigned to that layer.\n",
    "    Returns:\n",
    "        activated_result -- The result of the dot product and the activation\n",
    "        function\n",
    "    \"\"\" \n",
    "    return self.calculate_feed_forward(inputs,self.params)\n",
    "  \n",
    "def calculate_feed_forward(self, inputs, store):\n",
    "    for i in range(len(self.layers)-1):\n",
    "        if i == 0:\n",
    "          Z_i = inputs.dot(store['W{}'.format(i+1)]) + store[\n",
    "                                      'b{}'.format(i+1)]\n",
    "          A_i = self.activation_functs[i](Z_i)\n",
    "        else:\n",
    "          Z_i = A_i.dot(store['W{}'.format(i+1)]) + store[\n",
    "                                    'b{}'.format(i+1)]\n",
    "          A_i = self.activation_functs[i](Z_i)\n",
    "        store['Z{}'.format(i+1)] = Z_i \n",
    "        store['A{}'.format(i+1)] = A_i  \n",
    "    y_hat = A_i\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we have 2 functions that serve the same purpose, calculating the forward pass. The second one is a refactor made afterwards due to the fact that (spoiler alert) I needed to use this code for calculating the forward pass for a genetic weight optimized neural network.\n",
    "The indexes to calculate backpropagation are Z and A.\n",
    "\n",
    "Z = w_i * inputs + b_i  \n",
    "A = activation_function(Z_i)\n",
    "\n",
    "The return of the function is the last A_i being the activated output of the last layer. This result, depending on the activation function can take different meanings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self, inputs, y, y_hat):\n",
    "    \"\"\"The backward pass to update the weights.\n",
    "    It computes the ammount in which the weights must be changed to match the \n",
    "    outputs by the calculation of the error that the neural network is making\n",
    "    at the moment.\n",
    "    Note: It only performs the backpropagation, not the weight updating.\n",
    "    \n",
    "    Returns:\n",
    "        z_delta -- The delta diference in which the weights should be modified\n",
    "    \"\"\"  \n",
    "    dl_wrt_yhat = -(np.divide(y, y_hat) - np.divide((1 - y),(1-y_hat)))\n",
    "    dl_wrt_sig = y_hat * (1-y_hat)\n",
    "    dl_wrt_z_i = dl_wrt_yhat * dl_wrt_sig\n",
    "    for i in range(len(self.layers)-1,0,-1):\n",
    "      if i != len(self.layers)-1:\n",
    "        dl_wrt_z_i = dl_wrt_A_j * sigmoid_der(self.params['Z{}'.format(i)])\n",
    "      if i > 1:\n",
    "        dl_wrt_A_j = dl_wrt_z_i.dot(self.params['W{}'.format(i)].T)\n",
    "        dl_wrt_w_i = self.params['A{}'.format(i-1)].T.dot(dl_wrt_z_i)\n",
    "      else:\n",
    "        dl_wrt_w_i = inputs.T.dot(dl_wrt_z_i)\n",
    "      dl_wrt_b_i = np.sum(dl_wrt_z_i, axis=0)\n",
    "      self.params['dl_wrt_w{}'.format(i)] = dl_wrt_w_i\n",
    "      self.params['dl_wrt_b{}'.format(i)] = dl_wrt_b_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we define the backpropagation operator, that takes the activated results of the neural network, the real values of the dataset that we pass into the training loop and the Z_i's and A_i's calculated above and calculated all the derivatives necessary to update the weights correctly. The necessity to calculate those derivatives comes from the type of technique that backpropagation is.  \n",
    "Since backpropagation is an Stochastic Gradient Descent method, we need to calculate the gradient (i.e. the slope of the function) and the gradient of a function is calculated as the derivative of that function.\n",
    "Once we calculate the gradient is time to make is descend, operation that we will make updating the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __weight_updating(self):\n",
    "    \"\"\"In this fucnction we update the weights of the neural network using the \n",
    "    derivatives calculated in the backpropagation pass.\n",
    "    \"\"\"\n",
    "    for i in range(len(self.layers)-1):\n",
    "      self.params['W{}'.format(i+1)] = self.params[\n",
    "                  'W{}'.format(i+1)] - self.learning_rate * self.params[\n",
    "                  'dl_wrt_w{}'.format(i+1)]\n",
    "      self.params['b{}'.format(i+1)] = self.params[\n",
    "                  'b{}'.format(i+1)] - self.learning_rate * self.params[\n",
    "                  'dl_wrt_b{}'.format(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we define the weight updating method, that takes the weights and biases and their respective derivative and update the weights moving them forward into the convergence area. This procedure is executed for each layer of the network, and it is performed from the first to the last layer although the order doesn't matter since we have stored all the derivatives.\n",
    "\n",
    "## Basic Neural Network Demo\n",
    "In this section we will making a demo to test if the neural network that we have built in fact optimizes the weights and reduces the value of the loss function in the course of the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 1.22760983],\n",
      "       [-0.76522897],\n",
      "       [ 1.17770312]]), 'b1': array([-0.15035313])}\n",
      "-----Training Process------\n",
      "--------LOSS VALUES--------\n",
      "[0.9209102583778608, 0.9007855144499435, 0.8812766039703068, 0.8623490723765888]\n",
      "...\n",
      "[0.2199382788228734, 0.21787288174165617, 0.21584304313975414, 0.21384790832632694]\n",
      "-----Training Finished-----\n",
      "{'W1': array([[ 0.44255866],\n",
      "       [ 2.8135129 ],\n",
      "       [-0.12578231]]), 'b1': array([-1.18553206]), 'Z1': array([[ 1.61447312],\n",
      "       [-1.30017129],\n",
      "       [-0.73194428],\n",
      "       [ 2.06134215],\n",
      "       [ 1.93998417]]), 'A1': array([[0.83403149],\n",
      "       [0.21413619],\n",
      "       [0.32476821],\n",
      "       [0.88708867],\n",
      "       [0.8743504 ]]), 'dl_wrt_w1': array([[ 0.08620729],\n",
      "       [-0.40452943],\n",
      "       [ 0.08848659]]), 'dl_wrt_b1': array([0.13437498])}\n"
     ]
    }
   ],
   "source": [
    "from neuroevolution.networks.basic_neural_network import BasicNeuralNetwork\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_set = np.array([[0,1,0],[0,0,1],[1,0,0],[1,1,0],[1,1,1]])\n",
    "    labels = np.array([[1,0,0,1,1]])\n",
    "    labels = labels.reshape(5,1)\n",
    "    nnet = BasicNeuralNetwork(layers=[3,1], input_size=3, num_of_classes= 1)\n",
    "    print(nnet.params)\n",
    "    print(\"-----Training Process------\")\n",
    "    nnet.train(feature_set,labels, 100)\n",
    "    print(\"--------LOSS VALUES--------\")\n",
    "    print(nnet.loss[1:5])\n",
    "    print(\"...\")\n",
    "    print(nnet.loss[-4:])\n",
    "    print(\"-----Training Finished-----\")\n",
    "    print(nnet.params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code cell we have made a demo showcasing the power of backpropagation. As seen in the code, we show the first set of parameters of the neural network and once we train it we also show how the weights, biases, Z's A's and derivatives values have changed.\n",
    "The code also prints the loss values at first, and the last ones, just to check how the objective function value decreases over time as we make further iterations into the gradient descent method.\n",
    "\n",
    "## Conclusion and further improvements\n",
    "In this notebook, we have explained how a basic neural network is set up from absolute scratch using the numpy library to make the matricial operations, speeding up the algorithm since numpy is written and compiled in C. As stated, this class is general for any structure that you may imagine, always taking into conderation that the net built will be a fully connected one. As we have seen during the course of this notebook, the basic neural network class is able to reduce the loss function my using forward and back propagation. Finally, I want to encourage the reader to play with the code in this notebook, changing the structure, the data fed into the network, or even colaborate in this repository.  \n",
    "As improvements or things that will change over time, adding more activation functions and derivatives would be a very interesting addition.  \n",
    "Support for more types of networks in addition to fully connected ones, is an interesting addition aswell.  \n",
    "To conclude, the basics of the neural networks and the training procedure are a mistery if you use libraries like tensorflow and keras, I hope that once you read this notebook some of this black magic is transferred to you and maybe ecourage to expeeriment with this structures or more complex ones.\n",
    "\n",
    "## References\n",
    "This notebook is a generic implementation of this medium post: https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
